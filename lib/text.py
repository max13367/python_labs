import re
from collections import Counter
from typing import Dict, List, Tuple

def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    if yo2e:
        text = text.replace("Ñ‘", "Ğµ").replace("Ğ", "Ğ•")
    if casefold:
        text = text.casefold()
    text = re.compile(r"[\t\r\n]+").sub(" ", text)
    text = re.compile(r"\s+").sub(" ", text).strip()
    return text

def tokenize(text: str) -> List[str]:
    return re.compile(r"[^\W_]+(?:-[^\W_]+)*", flags=re.UNICODE).findall(text)

def count_freq(tokens: List[str]) -> Dict[str, int]:
    return dict(Counter(tokens))

def top_n(freq: Dict[str, int], n: int = 5) -> List[Tuple[str, int]]:
    if n <= 0:
        return []
    return sorted(freq.items(), key=lambda kv: (-kv[1], kv[0]))[:n]

if __name__ == "__main__":
    assert normalize("ĞŸÑ€Ğ˜Ğ²Ğ•Ñ‚\nĞœĞ˜Ñ€\t") == "Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ Ğ¼Ğ¸Ñ€"
    assert normalize("Ñ‘Ğ¶Ğ¸Ğº, ĞĞ»ĞºĞ°") == "ĞµĞ¶Ğ¸Ğº, ĞµĞ»ĞºĞ°"
    assert normalize("Hello\r\nWorld") == "hello world"
    assert normalize(" Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ ") == "Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹"

    assert tokenize("Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ Ğ¼Ğ¸Ñ€") == ["Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚", "Ğ¼Ğ¸Ñ€"]
    assert tokenize("hello,world!!!") == ["hello", "world"]
    assert tokenize("Ğ¿Ğ¾-Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¼Ñƒ ĞºÑ€ÑƒÑ‚Ğ¾") == ["Ğ¿Ğ¾-Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¼Ñƒ", "ĞºÑ€ÑƒÑ‚Ğ¾"]
    assert tokenize("2025 Ğ³Ğ¾Ğ´") == ["2025", "Ğ³Ğ¾Ğ´"]
    assert tokenize("emoji ğŸ˜€ Ğ½Ğµ ÑĞ»Ğ¾Ğ²Ğ¾") == ["emoji", "Ğ½Ğµ", "ÑĞ»Ğ¾Ğ²Ğ¾"]

    freq = count_freq(["a", "b", "a", "c", "b", "a"])
    assert freq == {"a": 3, "b": 2, "c": 1}
    assert top_n(freq, 2) == [("a", 3), ("b", 2)]

    freq2 = count_freq(["bb", "aa", "bb", "aa", "cc"])
    assert top_n(freq2, 2) == [("aa", 2), ("bb", 2)]

    print("Ğ²ÑĞµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾")