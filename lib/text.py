import re
from collections import Counter
from typing import Dict, List, Tuple

# –£–ª—É—á—à–µ–Ω–Ω—ã–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è Unicode
_WORD_RE = re.compile(r"[^\W_]+(?:-[^\W_]+)*", flags=re.UNICODE)
_WS_NL_RE = re.compile(r"[\t\r\n]+")
_WS_RE = re.compile(r"\s+")

def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞:
    - –ø—Ä–∏ yo2e=True –∑–∞–º–µ–Ω—è–µ—Ç —ë/–Å -> –µ/–ï
    - –ø—Ä–∏ casefold=True –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ casefold()
    - –∑–∞–º–µ–Ω—è–µ—Ç \t, \r, \n –Ω–∞ –ø—Ä–æ–±–µ–ª
    - —Å—Ö–ª–æ–ø—ã–≤–∞–µ—Ç –≤—Å–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –æ–±—Ä–µ–∑–∞–µ—Ç –∫—Ä–∞—è
    """
    if yo2e:
        text = text.replace("—ë", "–µ").replace("–Å", "–ï")
    if casefold:
        text = text.casefold()
    text = _WS_NL_RE.sub(" ", text)
    text = _WS_RE.sub(" ", text).strip()
    return text

def tokenize(text: str) -> List[str]:
    """
    –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è:
    - —Å–ª–æ–≤–∞ —ç—Ç–æ —à–∞–±–ª–æ–Ω [^\W_]+(?:-[^\W_]+)*
    - —Ä–∞–±–æ—Ç–∞–µ—Ç —Å Unicode –±—É–∫–≤–∞–º–∏ (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞, –ª–∞—Ç–∏–Ω–∏—Ü–∞)
    - –¥–µ—Ñ–∏—Å —Ä–∞–∑—Ä–µ—à–µ–Ω —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞
    - —á–∏—Å–ª–∞ —Ç–æ–∂–µ —Å—á–∏—Ç–∞—é—Ç—Å—è —Å–ª–æ–≤–∞–º–∏
    """
    return _WORD_RE.findall(text)

def count_freq(tokens: List[str]) -> Dict[str, int]:
    """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ –∏–∑ —Å–ø–∏—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤."""
    return dict(Counter(tokens))

def top_n(freq: Dict[str, int], n: int = 5) -> List[Tuple[str, int]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø-N –ø–∞—Ä (—Å–ª–æ–≤–æ, —á–∞—Å—Ç–æ—Ç–∞), —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª—é—á—É (-—á–∞—Å—Ç–æ—Ç–∞, —Å–ª–æ–≤–æ).
    –ü—Ä–∏ n <= 0 –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.
    """
    if n <= 0:
        return []
    return sorted(freq.items(), key=lambda kv: (-kv[1], kv[0]))[:n]

if __name__ == "__main__":
    # –ú–∏–Ω–∏-—Ç–µ—Å—Ç—ã
    assert normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t") == "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"
    assert normalize("—ë–∂–∏–∫, –Å–ª–∫–∞") == "–µ–∂–∏–∫, –µ–ª–∫–∞"
    assert normalize("Hello\r\nWorld") == "hello world"
    assert normalize(" –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã ") == "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"

    assert tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä") == ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]
    assert tokenize("hello,world!!!") == ["hello", "world"]
    assert tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ") == ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]
    assert tokenize("2025 –≥–æ–¥") == ["2025", "–≥–æ–¥"]
    assert tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ") == ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]

    freq = count_freq(["a", "b", "a", "c", "b", "a"])
    assert freq == {"a": 3, "b": 2, "c": 1}
    assert top_n(freq, 2) == [("a", 3), ("b", 2)]

    freq2 = count_freq(["bb", "aa", "bb", "aa", "cc"])
    assert top_n(freq2, 2) == [("aa", 2), ("bb", 2)]

    print("OK")